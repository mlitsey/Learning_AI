{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categoricals and groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of categorical data types\n",
    "What are the main advantages of storing data explicitly as categorical types instead of object types?\n",
    "\n",
    "* Computations are faster\n",
    "* Categorical data require less space in memory\n",
    "* All of the above *\n",
    "* None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by multiple columns\n",
    "In this exercise, you will return to working with the Titanic dataset from Chapter 1 and use .groupby() to analyze the distribution of passengers who boarded the Titanic.\n",
    "\n",
    "The 'pclass' column identifies which class of ticket was purchased by the passenger and the 'embarked' column indicates at which of the three ports the passenger boarded the Titanic. 'S' stands for Southampton, England, 'C' for Cherbourg, France and 'Q' for Queenstown, Ireland.\n",
    "\n",
    "Your job is to first group by the 'pclass' column and count the number of rows in each class using the 'survived' column. You will then group by the 'embarked' and 'pclass' columns and count the number of passengers.\n",
    "\n",
    "The DataFrame has been pre-loaded as titanic.\n",
    "\n",
    "* Group by the 'pclass' column and save the result as by_class.\n",
    "* Aggregate the 'survived' column of by_class using .count(). Save the result as count_by_class.\n",
    "* Print count_by_class. This has been done for you.\n",
    "* Group titanic by the 'embarked' and 'pclass' columns. Save the result as by_mult.\n",
    "* Aggregate the 'survived' column of by_mult using .count(). Save the result as count_mult.\n",
    "* Print count_mult. This has been done for you, so hit 'Submit Answer' to view the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass\n",
      "1    323\n",
      "2    277\n",
      "3    709\n",
      "Name: survived, dtype: int64\n",
      "embarked  pclass\n",
      "C         1         141\n",
      "          2          28\n",
      "          3         101\n",
      "Q         1           3\n",
      "          2           7\n",
      "          3         113\n",
      "S         1         177\n",
      "          2         242\n",
      "          3         495\n",
      "Name: survived, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Grouping your data by certain columns like this and aggregating them by another column, in this case, 'survived', allows you to carefully examine your data for interesting insights.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv('datasets/titanic.csv')\n",
    "\n",
    "# Group titanic by 'pclass'\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Aggregate 'survived' column of by_class by count\n",
    "count_by_class = by_class['survived'].count()\n",
    "\n",
    "# Print count_by_class\n",
    "print(count_by_class)\n",
    "\n",
    "# Group titanic by 'embarked' and 'pclass'\n",
    "by_mult = titanic.groupby(['embarked', 'pclass'])\n",
    "\n",
    "# Aggregate 'survived' column of by_mult by count\n",
    "count_mult = by_mult['survived'].count()\n",
    "\n",
    "# Print count_mult\n",
    "print(count_mult)\n",
    "\n",
    "'''Grouping your data by certain columns like this and aggregating them by another column, in this case, 'survived', allows you to carefully examine your data for interesting insights.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by another series\n",
    "In this exercise, you'll use two data sets from [Gapminder.org](https://www.gapminder.org/) to investigate the average life expectancy (in years) at birth in 2010 for the 6 continental regions. To do this you'll read the life expectancy data per country into one pandas DataFrame and the association between country and region into another.\n",
    "\n",
    "By setting the index of both DataFrames to the country name, you'll then use the region information to group the countries in the life expectancy DataFrame and compute the mean value for 2010.\n",
    "\n",
    "The life expectancy CSV file is available to you in the variable life_fname and the regions filename is available in the variable regions_fname.\n",
    "\n",
    "* Read life_fname into a DataFrame called life and set the index to 'Country'.\n",
    "* Read regions_fname into a DataFrame called regions and set the index to 'Country'.\n",
    "* Group life by the region column of regions and store the result in life_by_region.\n",
    "* Print the mean over the 2010 column of life_by_region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year                   1964    1965    1966    1967    1968    1969    1970  \\\n",
      "Country                                                                       \n",
      "Afghanistan          33.639  34.152  34.662  35.170  35.674  36.172  36.663   \n",
      "Albania              65.475  65.863  66.122  66.316  66.500  66.702  66.948   \n",
      "Algeria              47.953  48.389  48.806  49.205  49.592  49.976  50.366   \n",
      "Angola               34.604  35.007  35.410  35.816  36.222  36.627  37.032   \n",
      "Antigua and Barbuda  63.775  64.149  64.511  64.865  65.213  65.558  65.898   \n",
      "\n",
      "Year                   1971    1972    1973   ...      2004    2005    2006  \\\n",
      "Country                                       ...                             \n",
      "Afghanistan          37.143  37.614  38.075   ...    56.583  57.071  57.582   \n",
      "Albania              67.251  67.595  67.966   ...    75.725  75.949  76.124   \n",
      "Algeria              50.767  51.195  51.670   ...    69.682  69.854  70.020   \n",
      "Angola               37.439  37.846  38.247   ...    48.036  48.572  49.041   \n",
      "Antigua and Barbuda  66.232  66.558  66.875   ...    74.355  74.544  74.729   \n",
      "\n",
      "Year                   2007    2008    2009    2010    2011    2012    2013  \n",
      "Country                                                                      \n",
      "Afghanistan          58.102  58.618  59.124  59.612  60.079  60.524  60.947  \n",
      "Albania              76.278  76.433  76.598  76.780  76.979  77.185  77.392  \n",
      "Algeria              70.180  70.332  70.477  70.615  70.747  70.874  71.000  \n",
      "Angola               49.471  49.882  50.286  50.689  51.094  51.498  51.899  \n",
      "Antigua and Barbuda  74.910  75.087  75.263  75.437  75.610  75.783  75.954  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "gapminder = pd.read_csv('datasets/gapminder_tidy.csv')\n",
    "\n",
    "life = gapminder.pivot(index='Country', columns='Year', values='life')\n",
    "\n",
    "print(life.head())\n",
    "# save to csv\n",
    "life.to_csv('./datasets/life_fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         region\n",
      "Country                                        \n",
      "Afghanistan                          South Asia\n",
      "Albania                   Europe & Central Asia\n",
      "Algeria              Middle East & North Africa\n",
      "Angola                       Sub-Saharan Africa\n",
      "Antigua and Barbuda                     America\n"
     ]
    }
   ],
   "source": [
    "gapminder = pd.read_csv('datasets/gapminder_tidy.csv')\n",
    "regions = gapminder[['Country', 'region']]\n",
    "regions = regions.set_index(['Country'])\n",
    "# get rid of duplicate Country data\n",
    "regions = regions[~regions.index.duplicated(keep='first')]\n",
    "print(regions.head())\n",
    "type(regions)\n",
    "# save to csv\n",
    "regions.to_csv('./datasets/regions_fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region\n",
      "America                       74.037350\n",
      "East Asia & Pacific           73.405750\n",
      "Europe & Central Asia         75.656387\n",
      "Middle East & North Africa    72.805333\n",
      "South Asia                    68.189750\n",
      "Sub-Saharan Africa            57.575080\n",
      "Name: 2010, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It looks like the average life expectancy (in years) at birth in 2010 was highest in Europe & Central Asia and lowest in Sub-Saharan Africa.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read life_fname into a DataFrame: life\n",
    "life = pd.read_csv('./datasets/life_fname', index_col='Country')\n",
    "\n",
    "# Read regions_fname into a DataFrame: regions\n",
    "regions = pd.read_csv('./datasets/regions_fname', index_col='Country')\n",
    "\n",
    "# Group life by regions['region']: life_by_region\n",
    "life_by_region = life.groupby(regions['region'])\n",
    "\n",
    "# Print the mean over the '2010' column of life_by_region\n",
    "print(life_by_region['2010'].mean())\n",
    "\n",
    "'''It looks like the average life expectancy (in years) at birth in 2010 was highest in Europe & Central Asia and lowest in Sub-Saharan Africa.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing multiple aggregates of multiple columns\n",
    "The .agg() method can be used with a tuple or list of aggregations as input. When applying multiple aggregations on multiple columns, the aggregated DataFrame has a multi-level column index.\n",
    "\n",
    "In this exercise, you're going to group passengers on the Titanic by 'pclass' and aggregate the 'age' and 'fare' columns by the functions 'max' and 'median'. You'll then use multi-level selection to find the oldest passenger per class and the median fare price per class.\n",
    "\n",
    "The DataFrame has been pre-loaded as titanic.\n",
    "\n",
    "* Group titanic by 'pclass' and save the result as by_class.\n",
    "* Select the 'age' and 'fare' columns from by_class and save the result as by_class_sub.\n",
    "* Aggregate by_class_sub using 'max' and 'median'. You'll have to pass 'max' and 'median' in the form of a list to .agg().\n",
    "* Use .loc[] to print all of the rows and the column specification ('age','max'). This has been done for you.\n",
    "* Use .loc[] to print all of the rows and the column specification ('fare','median')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read life_fname into a DataFrame: \n",
    "titanic = pd.read_csv('./datasets/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass\n",
      "1    80.0\n",
      "2    70.0\n",
      "3    74.0\n",
      "Name: (age, max), dtype: float64\n",
      "pclass\n",
      "1    60.0000\n",
      "2    15.0458\n",
      "3     8.0500\n",
      "Name: (fare, median), dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It isn't surprising that the highest median fare was for the 1st passenger class.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group titanic by 'pclass': by_class\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Select 'age' and 'fare'\n",
    "by_class_sub = by_class[['age','fare']]\n",
    "\n",
    "# Aggregate by_class_sub by 'max' and 'median': aggregated\n",
    "aggregated = by_class_sub.agg(['max','median'])\n",
    "\n",
    "# Print the maximum age in each class\n",
    "print(aggregated.loc[:, ('age','max')])\n",
    "\n",
    "# Print the median fare in each class\n",
    "print(aggregated.loc[:,('fare','median')])\n",
    "\n",
    "'''It isn't surprising that the highest median fare was for the 1st passenger class.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating on index levels/fields\n",
    "If you have a DataFrame with a multi-level row index, the individual levels can be used to perform the groupby. This allows advanced aggregation techniques to be applied along one or more levels in the index and across one or more columns.\n",
    "\n",
    "In this exercise you'll use the full Gapminder dataset which contains yearly values of life expectancy, population, child mortality (per 1,000) and per capita gross domestic product (GDP) for every country in the world from 1964 to 2013.\n",
    "\n",
    "Your job is to create a multi-level DataFrame of the columns 'Year', 'Region' and 'Country'. Next you'll group the DataFrame by the 'Year' and 'Region' levels. Finally, you'll apply a dictionary aggregation to compute the total population, spread of per capita GDP values and average child mortality rate.\n",
    "\n",
    "The Gapminder CSV file is available as 'gapminder.csv'.\n",
    "\n",
    "* Read 'gapminder.csv' into a DataFrame with index_col=['Year','region','Country']. Sort the index.\n",
    "* Group gapminder with a level of ['Year','region'] using its level parameter. Save the result as by_year_region.\n",
    "* Define the function spread which returns the maximum and minimum of an input series. This has been done for you.\n",
    "* Create a dictionary with 'population':'sum', 'child_mortality':'mean' and 'gdp':spread as aggregator. This has been done for you.\n",
    "* Use the aggregator dictionary to aggregate by_year_region. Save the result as aggregated.\n",
    "* Print the last 6 entries of aggregated. This has been done for you, so hit 'Submit Answer' to view the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 child_mortality    population       gdp\n",
      "Year region                                                             \n",
      "2013 America                           17.745833  9.629087e+08   49634.0\n",
      "     East Asia & Pacific               22.285714  2.244209e+09  134744.0\n",
      "     Europe & Central Asia              9.831875  8.968788e+08   86418.0\n",
      "     Middle East & North Africa        20.221500  4.030504e+08  128676.0\n",
      "     South Asia                        46.287500  1.701241e+09   11469.0\n",
      "     Sub-Saharan Africa                76.944490  9.205996e+08   32035.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Are you able to see any correlations between population, child_mortality, and gdp?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame and sort the index: gapminder\n",
    "gapminder = pd.read_csv('./datasets/gapminder_tidy.csv',index_col=['Year','region','Country']).sort_index()\n",
    "\n",
    "# Group gapminder by 'Year' and 'region': by_year_region\n",
    "by_year_region = gapminder.groupby(level=['Year','region'])\n",
    "\n",
    "# Define the function to compute spread: spread\n",
    "def spread(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "# Create the dictionary: aggregator\n",
    "aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}\n",
    "\n",
    "# Aggregate by_year_region using the dictionary: aggregated\n",
    "aggregated = by_year_region.agg(aggregator)\n",
    "\n",
    "# Print the last 6 entries of aggregated \n",
    "print(aggregated.tail(6))\n",
    "\n",
    "'''Are you able to see any correlations between population, child_mortality, and gdp?'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping on a function of the index\n",
    "Groupby operations can also be performed on transformations of the index values. In the case of a DateTimeIndex, we can extract portions of the datetime over which to group.\n",
    "\n",
    "In this exercise you'll read in a set of sample sales data from February 2015 and assign the 'Date' column as the index. Your job is to group the sales data by the day of the week and aggregate the sum of the 'Units' column.\n",
    "\n",
    "Is there a day of the week that is more popular for customers? To find out, you're going to use .strftime('%a') to transform the index datetime values to abbreviated days of the week.\n",
    "\n",
    "The sales data CSV file is available to you as 'sales.csv'.\n",
    "\n",
    "* Read 'sales.csv' into a DataFrame with index_col='Date' and parse_dates=True.\n",
    "* Create a groupby object with sales.index.strftime('%a') as input and assign it to by_day.\n",
    "* Aggregate the 'Units' column of by_day with the .sum() method. Save the result as units_sum.\n",
    "* Print units_sum. This has been done for you, so hit 'Submit Answer' to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon    48\n",
      "Sat     7\n",
      "Thu    59\n",
      "Tue    13\n",
      "Wed    48\n",
      "Name: Units, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It looks like Monday, Wednesday, and Thursday were the most popular days for customers!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file: sales\n",
    "sales = pd.read_csv('./datasets/sales-feb-2015.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Create a groupby object: by_day\n",
    "by_day = sales.groupby(sales.index.strftime('%a'))\n",
    "\n",
    "# Create sum: units_sum\n",
    "units_sum = by_day['Units'].sum()\n",
    "\n",
    "# Print units_sum\n",
    "print(units_sum)\n",
    "\n",
    "'''It looks like Monday, Wednesday, and Thursday were the most popular days for customers!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
